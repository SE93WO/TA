{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjpg4XW3CVfl"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "In a recurrent neural network we store the output activations from one or more of the layers of the network. Often these are hidden later activations. Then, the next time we feed an input example to the network, we include the previously-stored outputs as additional inputs. You can think of the additional inputs as being concatenated to the end of the “normal” inputs to the previous layer. For example, if a hidden layer had 10 regular input nodes and 128 hidden nodes in the layer, then it would actually have 138 total inputs (assuming you are feeding the layer’s outputs into itself à la Elman) rather than into another layer). Of course, the very first time you try to compute the output of the network you’ll need to fill in those extra 128 inputs with 0s or something.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*NKhwsOYNUT5xU7Pyf6Znhg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G23f-eKcCVfo"
   },
   "source": [
    "Now, even though RNNs are quite powerful, they suffer from  **Vanishing gradient problem ** which hinders them from using long term information, like they are good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results so we don't just use regular RNNs. Instead, we use a better variation of RNNs: **Long Short Term Networks(LSTM).**\n",
    "\n",
    "### What is Vanishing Gradient problem?\n",
    "Vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the partial derivative of the error function with respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training. As one example of the problem cause, traditional activation functions such as the hyperbolic tangent function have gradients in the range (0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the \"front\" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n while the front layers train very slowly.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1460/1*FWy4STsp8k0M5Yd8LifG_Q.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVTzTaLMCVfq"
   },
   "source": [
    "# LSTM\n",
    "An LSTM has a similar control flow as a recurrent neural network. It processes data passing on information as it propagates forward. The differences are the operations within the LSTM’s cells.  \n",
    "![image](https://miro.medium.com/max/1917/1*0f8r3Vd-i4ueYND1CUrhMA.png)\n",
    "These operations are used to allow the LSTM to keep or forget information. Now looking at these operations can get a little overwhelming so we’ll go over this step by step.  \n",
    "\n",
    "### Core Concept\n",
    "The core concept of LSTM’s are the cell state, and it’s various gates. The cell state act as a transport highway that transfers relative information all the way down the sequence chain. You can think of it as the “memory” of the network. The cell state, in theory, can carry relevant information throughout the processing of the sequence. So even information from the earlier time steps can make it’s way to later time steps, reducing the effects of short-term memory. As the cell state goes on its journey, information get’s added or removed to the cell state via gates. The gates are different neural networks that decide which information is allowed on the cell state. The gates can learn what information is relevant to keep or forget during training.\n",
    "\n",
    "\n",
    "### Sigmoid\n",
    "Gates contains sigmoid activations. A sigmoid activation is similar to the tanh activation. Instead of squishing values between -1 and 1, it squishes values between 0 and 1. That is helpful to update or forget data because any number getting multiplied by 0 is 0, causing values to disappears or be “forgotten.” Any number multiplied by 1 is the same value therefore that value stay’s the same or is “kept.” The network can learn which data is not important therefore can be forgotten or which data is important to keep.\n",
    "![image](https://miro.medium.com/max/1282/1*rOFozAke2DX5BmsX2ubovw.gif)\n",
    "Let’s dig a little deeper into what the various gates are doing, shall we? So we have three different gates that regulate information flow in an LSTM cell. A forget gate, input gate, and output gate.\n",
    "\n",
    "### Forget gate\n",
    "First, we have the forget gate. This gate decides what information should be thrown away or kept. Information from the previous hidden state and information from the current input is passed through the sigmoid function. Values come out between 0 and 1. The closer to 0 means to forget, and the closer to 1 means to keep.\n",
    "![image](https://miro.medium.com/max/1282/1*GjehOa513_BgpDDP6Vkw2Q.gif)\n",
    "\n",
    "### Input Gate\n",
    "To update the cell state, we have the input gate. First, we pass the previous hidden state and current input into a sigmoid function. That decides which values will be updated by transforming the values to be between 0 and 1. 0 means not important, and 1 means important. You also pass the hidden state and current input into the tanh function to squish values between -1 and 1 to help regulate the network. Then you multiply the tanh output with the sigmoid output. The sigmoid output will decide which information is important to keep from the tanh output.\n",
    "![image](https://miro.medium.com/max/1282/1*TTmYy7Sy8uUXxUXfzmoKbA.gif)\n",
    "\n",
    "### Cell State\n",
    "Now we should have enough information to calculate the cell state. First, the cell state gets pointwise multiplied by the forget vector. This has a possibility of dropping values in the cell state if it gets multiplied by values near 0. Then we take the output from the input gate and do a pointwise addition which updates the cell state to new values that the neural network finds relevant. That gives us our new cell state.\n",
    "![image](https://miro.medium.com/max/1282/1*S0rXIeO_VoUVOyrYHckUWg.gif)\n",
    "\n",
    "### Output Gate\n",
    "Last we have the output gate. The output gate decides what the next hidden state should be. Remember that the hidden state contains information on previous inputs. The hidden state is also used for predictions. First, we pass the previous hidden state and the current input into a sigmoid function. Then we pass the newly modified cell state to the tanh function. We multiply the tanh output with the sigmoid output to decide what information the hidden state should carry. The output is the hidden state. The new cell state and the new hidden is then carried over to the next time step.\n",
    "![image](https://miro.medium.com/max/1282/1*VOXRGhOShoWWks6ouoDN3Q.gif)\n",
    "\n",
    "To review, the Forget gate decides what is relevant to keep from prior steps. The input gate decides what information is relevant to add from the current step. The output gate determines what the next hidden state should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inyjr7wU0DZ2"
   },
   "source": [
    "# Description\n",
    "Twitter has become an important communication channel in times of emergency.\n",
    "The ubiquitousness of smartphones enables people to announce an emergency they’re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n",
    "But, it’s not always clear whether a person’s words are actually announcing a disaster.   \n",
    "![image](https://storage.googleapis.com/kaggle-media/competitions/tweet_screenshot.png)   \n",
    "The author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it’s less clear to a machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zS7CUwgbSvhG"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "KCYLoNj1CVft",
    "outputId": "520a6de2-4302-456c-c0b3-5ce81b1908c0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import Input, Dense, Embedding, LSTM, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sXLTKLfqTBcr"
   },
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "XN8GzPIACVfz",
    "outputId": "45fb4039-d7e6-4161-f898-6fa92cd95b0d"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omkSNtZQTaZ9"
   },
   "source": [
    "# Columns\n",
    "* id - a unique identifier for each tweet\n",
    "* text - the text of the tweet\n",
    "* location - the location the tweet was sent from (may be blank)\n",
    "* keyword - a particular keyword from the tweet (may be blank)\n",
    "* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "TQ-6BpbgCVf2",
    "outputId": "b0cdb454-c728-4c35-9f2d-99ba32275dae"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OoMUDk5uiP9Z"
   },
   "source": [
    "# Hyperparametrs\n",
    "* embedded_size : This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "* max_features : This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger.\n",
    "* maxlen : This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HjKnkS9CVf6"
   },
   "outputs": [],
   "source": [
    "embedded_size = 100\n",
    "max_features = 10000\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gH6S61qzkZDz"
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qkRy7Sn8CVgA"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(train,test_size = 0.1 , random_state = 43)\n",
    "train_X = train_df.text.values\n",
    "test_X = test.text.values\n",
    "val_X = val_df.text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jr0qTbWhS4e"
   },
   "source": [
    "# Tokenization\n",
    "![image](https://media.geeksforgeeks.org/wp-content/cdn-uploads/StringTokenizer.png)\n",
    "\n",
    "I want you to think about the English language here. Pick up any sentence you can think of and hold that in your mind as you read this section. This will help you understand the importance of tokenization in a much easier manner.\n",
    "\n",
    "Before processing a natural language, we need to identify the words that constitute a string of characters. That’s why tokenization is the most basic step to proceed with NLP (text data). This is important because the meaning of the text could easily be interpreted by analyzing the words present in the text.\n",
    "\n",
    "Let’s take an example. Consider the below string:\n",
    "\n",
    "“This is a cat.”\n",
    "\n",
    "What do you think will happen after we perform tokenization on this string? We get [‘This’, ‘is’, ‘a’, cat’].\n",
    "\n",
    "There are numerous uses of doing this. We can use this tokenized form to:\n",
    "\n",
    "* Count the number of words in the text\n",
    "* Count the frequency of the word, that is, the number of times a particular word is present\n",
    "And so on. We can extract a lot more information which we’ll discuss in detail in future articles. For now, it’s time to dive into the meat of this article – the different methods of performing tokenization in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c5PmfSZzCVgE"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsPn-KsICVgH"
   },
   "outputs": [],
   "source": [
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8xvxbjVJCVgK"
   },
   "outputs": [],
   "source": [
    "train_y = train_df.target.values\n",
    "val_y = val_df.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFeFCVc8xs95"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "colab_type": "code",
    "id": "mMKlQvp0CVgN",
    "outputId": "93cc1245-ab6d-4ebd-c106-7b1afc93fec3"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "inp = Input(shape = (maxlen,))\n",
    "x = Embedding(max_features,embedded_size)(inp)\n",
    "x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16,activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1,activation = 'sigmoid')(x)\n",
    "model=Model(inputs = inp,outputs = x)\n",
    "model.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "colab_type": "code",
    "id": "GlKxaGzHCVgQ",
    "outputId": "8e88ce33-b5dc-493b-b057-f5b5fedfca97"
   },
   "outputs": [],
   "source": [
    "model.fit(train_X, train_y, batch_size=512, epochs=10, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tnUHQITtCVgW",
    "outputId": "7e6cb030-5881-46ef-ecdf-3e0613945e28"
   },
   "outputs": [],
   "source": [
    "preds = model.predict([test_X],batch_size = 1024,verbose = 1)\n",
    "predictions = (preds > 0.5).astype(int)\n",
    "predictions = np.ndarray.flatten(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yX4qoRYtxhfH"
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv('socialmedia-disaster-tweets.csv')\n",
    "target = target[['choose_one', 'text']]\n",
    "target['target'] = (target['choose_one']=='Relevant').astype(int)\n",
    "target['id'] = target.index\n",
    "merged_df = pd.merge(test, target, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "colab_type": "code",
    "id": "gFRUnuuzoScl",
    "outputId": "d22a9b28-090f-4451-bf72-399aa4bb2205"
   },
   "outputs": [],
   "source": [
    "real = []\n",
    "text = []\n",
    "pred = []\n",
    "for i in range(10):\n",
    "  rand= np.random.randint(0,len(preds))\n",
    "  real.append(merged_df['target'][rand])\n",
    "  text.append(test['text'][rand])\n",
    "  pred.append(preds[rand] * 100)\n",
    "\n",
    "dataset = pd.DataFrame({'Real':real, 'Pred': pred, 'Text':text})\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "tBEjJy1s3tjq",
    "outputId": "2da558d0-cb05-4db5-9a0f-41b6290e86da"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = merged_df['target'].values.tolist()\n",
    "acc = accuracy_score(acc, np.round(preds))*100\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
